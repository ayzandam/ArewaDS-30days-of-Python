{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise for Day 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama Speech:\n",
      "Number of Lines:  66\n",
      "Number of words  13341\n",
      "\n",
      "\n",
      "Michelle Obama Speech:\n",
      "Number of Lines:  83\n",
      "Number of words  11789\n",
      "\n",
      "\n",
      "Donald Speech:\n",
      "Number of Lines:  48\n",
      "Number of words  7275\n",
      "\n",
      "\n",
      "Melina Trump Speech:\n",
      "Number of Lines:  33\n",
      "Number of words  7489\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a function which count number of lines and number of words in a text. \n",
    "# All the files are in the data the folder: \n",
    "# a) Read obama_speech.txt file and count number of lines and words \n",
    "\n",
    "with open('./obama_speech.txt', 'r') as f:\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "    for line in f:\n",
    "        num_lines += 1\n",
    "        num_words += len(line)\n",
    "\n",
    "    print(\"Obama Speech:\")\n",
    "    print(\"Number of Lines: \", num_lines)\n",
    "    print(\"Number of words \", num_words)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# b) Read michelle_obama_speech.txt file and count number of lines and words \n",
    "\n",
    "with open('./michelle_obama_speech.txt', 'r') as f:\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "    for line in f:\n",
    "        num_lines += 1\n",
    "        num_words += len(line)\n",
    "\n",
    "    print(\"Michelle Obama Speech:\")\n",
    "    print(\"Number of Lines: \", num_lines)\n",
    "    print(\"Number of words \", num_words)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# c) Read donald_speech.txt file and count number of lines and words\n",
    "\n",
    "with open('./donald_speech.txt', 'r') as f:\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "    for line in f:\n",
    "        num_lines += 1\n",
    "        num_words += len(line)\n",
    "\n",
    "    print(\"Donald Speech:\")\n",
    "    print(\"Number of Lines: \", num_lines)\n",
    "    print(\"Number of words \", num_words)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# d) Read melina_trump_speech.txt file and count number of lines and words\n",
    "\n",
    "with open('./melina_trump_speech.txt', 'r') as f:\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "    for line in f:\n",
    "        num_lines += 1\n",
    "        num_words += len(line)\n",
    "\n",
    "    print(\"Melina Trump Speech:\")\n",
    "    print(\"Number of Lines: \", num_lines)\n",
    "    print(\"Number of words \", num_words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('English', 91), ('French', 45), ('Arabic', 25), ('Spanish', 24), ('Russian', 9), ('Portuguese', 9), ('Dutch', 8), ('German', 7), ('Chinese', 5), ('Swahili', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Read the countries_data.json data file in data directory, \n",
    "# create a function that finds the ten most spoken languages\n",
    "import json\n",
    "\n",
    "def most_spoken_lang(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        all_languages = [language for country in data for language in country.get('languages', [])]\n",
    "        language_count = {language: all_languages.count(language) for language in set(all_languages)}\n",
    "\n",
    "        most_spoken_languages = sorted(language_count.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        return most_spoken_languages\n",
    "\n",
    "file_path = './countries_data.json'\n",
    "print(most_spoken_lang(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('China', 1377422166), ('India', 1295210000), ('United States of America', 323947000), ('Indonesia', 258705000), ('Brazil', 206135893), ('Pakistan', 194125062), ('Nigeria', 186988000), ('Bangladesh', 161006790), ('Russian Federation', 146599183), ('Japan', 126960000)]\n"
     ]
    }
   ],
   "source": [
    "# Read the countries_data.json data file in data directory, \n",
    "# create a function that creates a list of the ten most populated countries\n",
    "\n",
    "import json\n",
    "\n",
    "def most_populous(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        country_population = {country['name']: country['population'] for country in data}\n",
    "        most_populated_countries = sorted(country_population.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        return most_populated_countries\n",
    "\n",
    "file_path = './countries_data.json'\n",
    "print(most_populous(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stephen.marquard@uct.ac.za'], ['postmaster@collab.sakaiproject.org'], ['200801051412.m05ECIaH010327@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['stephen.marquard@uct.ac.za'], ['source@collab.sakaiproject.org'], ['stephen.marquard@uct.ac.za'], ['stephen.marquard@uct.ac.za'], ['louis@media.berkeley.edu'], ['postmaster@collab.sakaiproject.org'], ['200801042308.m04N8v6O008125@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['louis@media.berkeley.edu'], ['source@collab.sakaiproject.org'], ['louis@media.berkeley.edu'], ['louis@media.berkeley.edu'], ['zqian@umich.edu'], ['postmaster@collab.sakaiproject.org'], ['200801042109.m04L92hb007923@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['zqian@umich.edu'], ['source@collab.sakaiproject.org'], ['zqian@umich.edu'], ['zqian@umich.edu'], ['rjlowe@iupui.edu'], ['postmaster@collab.sakaiproject.org'], ['200801042044.m04Kiem3007881@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['rjlowe@iupui.edu'], ['source@collab.sakaiproject.org'], ['rjlowe@iupui.edu'], ['rjlowe@iupui.edu'], ['zqian@umich.edu'], ['postmaster@collab.sakaiproject.org'], ['200801042001.m04K1cO0007738@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['zqian@umich.edu'], ['source@collab.sakaiproject.org'], ['zqian@umich.edu'], ['zqian@umich.edu'], ['zqian@umich.edu'], ['rjlowe@iupui.edu'], ['postmaster@collab.sakaiproject.org'], ['200801041948.m04JmdwO007705@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['rjlowe@iupui.edu'], ['source@collab.sakaiproject.org'], ['rjlowe@iupui.edu'], ['rjlowe@iupui.edu'], ['cwen@iupui.edu'], ['postmaster@collab.sakaiproject.org'], ['200801041635.m04GZQGZ007313@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['cwen@iupui.edu'], ['hu2@iupui.edu'], ['cwen@iupui.edu'], ['postmaster@collab.sakaiproject.org'], ['200801041633.m04GX6eG007292@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['cwen@iupui.edu'], ['hu2@iupui.edu'], ['gsilver@umich.edu'], ['postmaster@collab.sakaiproject.org'], ['200801041611.m04GB1Lb007221@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['gsilver@umich.edu'], ['source@collab.sakaiproject.org'], ['gsilver@umich.edu'], ['gsilver@umich.edu'], ['gsilver@umich.edu'], ['postmaster@collab.sakaiproject.org'], ['200801041610.m04GA5KP007209@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['gsilver@umich.edu'], ['source@collab.sakaiproject.org'], ['gsilver@umich.edu'], ['gsilver@umich.edu'], ['zqian@umich.edu'], ['postmaster@collab.sakaiproject.org'], ['200801041609.m04G9EuX007197@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['zqian@umich.edu'], ['source@collab.sakaiproject.org'], ['zqian@umich.edu'], ['zqian@umich.edu'], ['gsilver@umich.edu'], ['postmaster@collab.sakaiproject.org'], ['200801041608.m04G8d7w007184@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['gsilver@umich.edu'], ['source@collab.sakaiproject.org'], ['gsilver@umich.edu'], ['gsilver@umich.edu'], ['wagnermr@iupui.edu'], ['postmaster@collab.sakaiproject.org'], ['200801041537.m04Fb6Ci007092@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['wagnermr@iupui.edu'], ['source@collab.sakaiproject.org'], ['wagnermr@iupui.edu'], ['wagnermr@iupui.edu'], ['zqian@umich.edu'], ['postmaster@collab.sakaiproject.org'], ['200801041515.m04FFv42007050@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['zqian@umich.edu'], ['source@collab.sakaiproject.org'], ['zqian@umich.edu'], ['zqian@umich.edu'], ['antranig@caret.cam.ac.uk'], ['postmaster@collab.sakaiproject.org'], ['200801041502.m04F21Jo007031@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['antranig@caret.cam.ac.uk'], ['source@collab.sakaiproject.org'], ['antranig@caret.cam.ac.uk'], ['antranig@caret.cam.ac.uk'], ['gopal.ramasammycook@gmail.com'], ['postmaster@collab.sakaiproject.org'], ['200801041403.m04E3psW006926@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['gopal.ramasammycook@gmail.com'], ['source@collab.sakaiproject.org'], ['gopal.ramasammycook@gmail.com'], ['gopal.ramasammycook@gmail.com'], ['david.horwitz@uct.ac.za'], ['postmaster@collab.sakaiproject.org'], ['200801041200.m04C0gfK006793@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['david.horwitz@uct.ac.za'], ['source@collab.sakaiproject.org'], ['david.horwitz@uct.ac.za'], ['david.horwitz@uct.ac.za'], ['david.horwitz@uct.ac.za'], ['david.horwitz@uct.ac.za'], ['postmaster@collab.sakaiproject.org'], ['200801041106.m04B6lK3006677@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['david.horwitz@uct.ac.za'], ['source@collab.sakaiproject.org'], ['david.horwitz@uct.ac.za'], ['david.horwitz@uct.ac.za'], ['david.horwitz@uct.ac.za'], ['postmaster@collab.sakaiproject.org'], ['200801040947.m049lUxo006517@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['david.horwitz@uct.ac.za'], ['source@collab.sakaiproject.org'], ['david.horwitz@uct.ac.za'], ['david.horwitz@uct.ac.za'], ['josrodri@iupui.edu'], ['david.horwitz@uct.ac.za'], ['postmaster@collab.sakaiproject.org'], ['200801040932.m049W2i5006493@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['david.horwitz@uct.ac.za'], ['source@collab.sakaiproject.org'], ['david.horwitz@uct.ac.za'], ['david.horwitz@uct.ac.za'], ['josrodri@iupui.edu'], ['stephen.marquard@uct.ac.za'], ['postmaster@collab.sakaiproject.org'], ['200801040905.m0495rWB006420@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['stephen.marquard@uct.ac.za'], ['source@collab.sakaiproject.org'], ['stephen.marquard@uct.ac.za'], ['stephen.marquard@uct.ac.za'], ['louis@media.berkeley.edu'], ['postmaster@collab.sakaiproject.org'], ['200801040023.m040NpCc005473@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['louis@media.berkeley.edu'], ['source@collab.sakaiproject.org'], ['louis@media.berkeley.edu'], ['louis@media.berkeley.edu'], ['louis@media.berkeley.edu'], ['postmaster@collab.sakaiproject.org'], ['200801032216.m03MGhDa005292@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['louis@media.berkeley.edu'], ['source@collab.sakaiproject.org'], ['louis@media.berkeley.edu'], ['louis@media.berkeley.edu'], ['ray@media.berkeley.edu'], ['postmaster@collab.sakaiproject.org'], ['200801032205.m03M5Ea7005273@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['ray@media.berkeley.edu'], ['source@collab.sakaiproject.org'], ['ray@media.berkeley.edu'], ['ray@media.berkeley.edu'], ['cwen@iupui.edu'], ['postmaster@collab.sakaiproject.org'], ['200801032133.m03LX3gG005191@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['cwen@iupui.edu'], ['cwen@iupui.edu'], ['postmaster@collab.sakaiproject.org'], ['200801032127.m03LRUqH005177@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['cwen@iupui.edu'], ['wagnermr@iupui.edu'], ['cwen@iupui.edu'], ['postmaster@collab.sakaiproject.org'], ['200801032122.m03LMFo4005148@nakamura.uits.iupui.edu'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['source@collab.sakaiproject.org'], ['cwen@iupui.edu'], ['cwen@iupui.edu'], ['wagnermr@iupui.edu']]\n"
     ]
    }
   ],
   "source": [
    "# Extract all incoming email addresses as a list from the email_exchange_big.txt file.\n",
    "import re\n",
    "\n",
    "with open('./email_exchange_big.txt', 'r') as f:\n",
    "    all_emails = []\n",
    "    for line in f:\n",
    "        pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "        emails = re.findall(pattern, line)\n",
    "        if len(emails) == 0:\n",
    "           continue\n",
    "        else:\n",
    "            all_emails.append(emails)\n",
    "    print(all_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('sample', 2), ('statement', 1), ('just', 1), ('the', 1)]\n",
      "[('a', 2), ('sample', 2), ('statement', 1), ('just', 1), ('the', 1), ('show', 1), ('in', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Find the most common words in the English language. \n",
    "# Call the name of your function find_most_common_words, \n",
    "# it will take two parameters - a string or a file and a positive integer, indicating the number of words. \n",
    "# Your function will return an array of tuples in descending order.\n",
    "import os \n",
    "def find_most_common_words(file_path, num):\n",
    "    words = []\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                words.extend([word for word in line.split(\" \")])\n",
    "    else:\n",
    "        words.extend([word for word in file_path.split(\" \")])\n",
    "\n",
    "    freq = {i:words.count(i) for i in set(words)}\n",
    "    sorted_freq = sorted(freq.items(), key=lambda x:x[1], reverse=True)[:num]\n",
    "    return sorted_freq\n",
    "\n",
    "print(find_most_common_words(\"This is a sample statement just to show a sample in the text\", 5))\n",
    "print(find_most_common_words(\"This is a sample statement just to show a sample in the text\", 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama speech ten most common words:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 120), ('and', 107), ('of', 81), ('to', 66), ('our', 58), ('we', 50), ('a', 48), ('that', 47), ('is', 36), ('\\n', 31)]\n",
      "\n",
      "\n",
      "Michelle speech ten most common words:\n",
      "[('to', 83), ('and', 80), ('the', 78), ('of', 46), ('â€”', 41), ('\\n', 41), ('a', 41), ('that', 40), ('in', 36), ('our', 27)]\n",
      "\n",
      "\n",
      "Trump speech ten most common words:\n",
      "[('the', 61), ('and', 53), ('will', 40), ('of', 38), ('to', 32), ('our', 30), ('we', 26), ('is', 20), ('We', 15), ('America', 14)]\n",
      "\n",
      "\n",
      "Melina speech ten most common words:\n",
      "[('and', 73), ('to', 54), ('the', 48), ('I', 28), ('is', 28), ('for', 27), ('of', 25), ('a', 22), ('that', 19), ('in', 17)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the function, find_most_frequent_words to find: \n",
    "# a) The ten most frequent words used in Obama's speech \n",
    "print(\"Obama speech ten most common words:\")\n",
    "print(find_most_common_words(\"./obama_speech.txt\", 10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# b) The ten most frequent words used in Michelle's speech \n",
    "print(\"Michelle speech ten most common words:\")\n",
    "print(find_most_common_words(\"./michelle_obama_speech.txt\", 10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# c) The ten most frequent words used in Trump's speech \n",
    "print(\"Trump speech ten most common words:\")\n",
    "print(find_most_common_words(\"./donald_speech.txt\", 10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# d) The ten most frequent words used in Melina's speech\n",
    "print(\"Melina speech ten most common words:\")\n",
    "print(find_most_common_words(\"./melina_trump_speech.txt\", 10))\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two transcript are not similar\n",
      "The two transcript are similar\n"
     ]
    }
   ],
   "source": [
    "# Write a python application that checks similarity between two texts. \n",
    "# It takes a file or a string as a parameter and it will evaluate the similarity of the two texts. \n",
    "# For instance check the similarity between the transcripts of Michelle's and Melina's speech. \n",
    "# You may need a couple of functions, function to clean the text(clean_text), \n",
    "# function to remove support words(remove_support_words) and finally to check the similarity(check_text_similarity). \n",
    "# List of stop words are in the data directory\n",
    "import os\n",
    "import re\n",
    "\n",
    "def similarity_checker(file1_path, file2_path):\n",
    "    file1_content = []\n",
    "    file2_content = []\n",
    "\n",
    "    if os.path.isfile(file1_path) and os.path.isfile(file2_path):\n",
    "        with open(file1_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                file1_content.extend(line)\n",
    "\n",
    "        with open(file2_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                file2_content.extend(line)\n",
    "    \n",
    "    else:\n",
    "        file1_content.extend(file1_path)\n",
    "        file2_content.extend(file2_path)\n",
    "\n",
    "    # Function to remove clean text\n",
    "    def clean_text(text1, text2):\n",
    "        txt1 = [re.sub('[.,;\\']', '', word).lower() for line in text1 for word in line]\n",
    "        txt2 = [re.sub('[.,;\\']', '', word).lower() for line in text2 for word in line]\n",
    "\n",
    "        return txt1, txt2\n",
    "    \n",
    "    tran1, tran2 = clean_text(file1_content, file2_content)\n",
    "\n",
    "    # Function to remove stopwords \n",
    "    def remove_stopwords(text1, text2):\n",
    "        stop_words = ['i','me','my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up','down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "        txt1 = [word for line in text1 for word in line.split(\" \") if word not in stop_words]\n",
    "        txt2 = [word for line in text2 for word in line.split(\" \") if word not in stop_words]\n",
    "\n",
    "        return txt1, txt2\n",
    "    \n",
    "    # Check for similarity\n",
    "    transcript1, transcript2 = remove_stopwords(tran1, tran2)\n",
    "\n",
    "    if transcript1 == transcript2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "print(\"The two transcript are similar\") if similarity_checker('./michelle_obama_speech.txt', './melina_trump_speech.txt') else print(\"The two transcript are not similar\")\n",
    "print(\"The two transcript are similar\") if similarity_checker('./melina_trump_speech.txt', './melina_trump_speech.txt') else print(\"The two transcript are not similar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most repeated word are:  [('', 15411), ('\\n', 1237), ('the', 737), ('I', 537), ('and', 530), ('to', 513), ('of', 478), ('a', 438), ('in', 325), ('is', 313)]\n"
     ]
    }
   ],
   "source": [
    "# Find the 10 most repeated words in the romeo_and_juliet.txt\n",
    "\n",
    "def most_repeated(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            words.extend([word for word in line.split(\" \")])\n",
    "\n",
    "        freq = {i:words.count(i) for i in set(words)}\n",
    "        sorted_freq = sorted(freq.items(), key=lambda x:x[1], reverse=True)[:10]\n",
    "        return sorted_freq\n",
    "    \n",
    "print(\"The 10 most repeated word are: \", most_repeated('./romeo_and_juliet.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the hacker news csv file and find out: \n",
    "# a) Count the number of lines containing python or Python \n",
    "# b) Count the number lines containing JavaScript, javascript or Javascript \n",
    "# c) Count the number lines containing Java and not JavaScript\n",
    "import csv\n",
    "\n",
    "def  count_words(file_path):\n",
    "    python_count = 0\n",
    "    javascript_count = 0\n",
    "    java_count = 0\n",
    "\n",
    "    with open(file_path, 'r') as csv:\n",
    "        data = csv.reader(csv, delimiter=',')\n",
    "        for row in data:\n",
    "            if bool(re.match(r'P|python', row[0])):\n",
    "                python_count += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        for row in data:\n",
    "            if bool(re.match(r'J|javaS|script', row[0])):\n",
    "                javascript_count += 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        for row in data:\n",
    "            if bool(re.match(r'Java', row[0])):\n",
    "                java_count += 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        return python_count, javascript_count, java_count\n",
    "\n",
    "\n",
    "# print(\"Number of counts: Python: {}, Javascript {}, Java {}\", count_words(\"./path\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
